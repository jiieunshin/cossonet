% !TeX root = RJwrapper.tex
\title{cossonet: An Integrated R Package for Fast Sparse Nonparametric Regression for High-dimensional data}


\author{by Jieun Shin and Changyi Park}

\maketitle

\abstract{%
The package \pkg{cossonet} offers the practical function of COSSO, a nonparametric regression model based on ANOVA decomposition. \pkg{cossonet} has several improvements for general usability in high-dimensional data. The package \pkg{cossonet} eliminates matrix inversion in the COSSO algorithm by employing the coordinate descent algorithm to fast computation. It supports various response variables belonging to the exponential family, and it is extended with the elastic-net penalty to select correlated components more effectively. We show simulation and real datasets analysis to varify its usability.
}

\hypertarget{sec1}{%
\section{Introduction}\label{sec1}}

With the rapid development of data collection technology, the amount and complexity of data in many fields have increased exponentially. In high-dimensional data which is the number of predictors \(d\) exceeds the number of observations \(n\), one of the goals of statistical modeling is to identify complex relationships between response and explanatory variables. One simple approach to achieve this goal is linear regression, but it does not easily provide well-defined coefficient estimates \citep{Montgomery:2021}. In high-dimensional data, model selection is essential because in many cases there are only a few variables that affect predictive performance. Statistical variable selection methods such as forward selection, backward elimination, and stepwise procedures are basic approaches, while regularization methods such as LASSO \citep{Tibshirani:1996}, ridge \citep{Hoerl:1970}, SCAD \citep{Fan:2001}, and elastic-net \citep{Zou:2005} are also widely used.

Unfortunately, the expectation that explanatory variables linearly explain the response variable is often unrealistic. Nonparametric models are good alternative because they allow nonlinear relationships and are more flexible than parametric models. Several widely used nonparametric models, such as CART \citep{Breiman:1984}, MARS \citep{Friedman:1991}, BRUTO \citep{Hastie:1989}, and TURBO \citep{Friedman:1989}, employ greedy algorithms to find local optima, similar to the forward selection and backward elimination of linear regression.

Smoothing spline ANOVA (SS-ANOVA) is a widely used method for multivariate function estimation \citep[Gu:2013]{Wahba:1990}. Recently, \citet{Lin:2006} proposed the COmponent Selection and Smoothing Operator (COSSO), a nonparametric regression model that performs model selection and estimation simultaneously, and is applied in various fields \citep[Dempsey:2017]{Kavuri:2017}. COSSO introduces a new LASSO-type penalty that consists of the sum of the norms of the components. The COSSO penalty is computationally useful for high-dimensional data because the ANOVA decomposition approximates high dimensions as a sum of low dimensions. This penalty estimates a sparse component similar to LASSO, which is a global optimum. \citet{Lin:2006} showed that LASSO is a special case of the COSSO penalty. After \citet{Lin:2006} introduced COSSO for Gaussian regression, \citet{Zhang:2006} and \citet{Leng:2006} extended it to the exponential family and the Cox proportional hazards (CoxPH) model, respectively.

In this paper, we introduce \pkg{cossonet} which is an integrated R package for fast sparse nonparametric regression for high-dimensional data. \pkg{cossonet} is a function integrating models of \citet{Zhang:2006}, \citet{Lin:2006}, and \citet{Leng:2006}. \pkg{cossonet} provides three key contributions. First, the standard LASSO-type penalty is extended to an elastic-net type to select correlated components. Second, \pkg{cossonet} aims to achieve fast computation for \(n\) and \(d\). The existing studies derived algorithms to solve the inverse matrix for \(n\) and \(d\). To reduce the computational burden for large \(n\), they used an alternative algorithm based on subset of \(m < n\). \pkg{cossonet} is implemented as a coordinate descent algorithm under the subset-based algorithm for fast computations for both large \(n\) and \(d\). Finally, \pkg{cossonet} can deal with a various response types including continuous, binary classes, non-negative counts, and survival. A comparison for the R package \pkg{cossonet} and \CRANpkg{cosso} can be seen in Table \ref{tab:tab-comp-latex}.

This paper is organized as follows. Section \texttt{\textbackslash{}\textbackslash{}@ref(sec2)} provides an introduction to the SS-ANOVA framework, which serves as the foundation for the COSSO model, and defines COSSO with elastic-net penalty for the exponential family and CoxPH models. In Section @(sec3), coordinate descent algorithms are derived for the regression models, along with a description of smoothing spline selection via cross-validation. Section \ref{sec4} presents the usage of the \pkg{cossonet} package and analyzes simulation results obtained with \pkg{cossonet}. Real data analysis and discussion of the results are included in Section \ref{sec5}. Finally, Section \ref{sec6} provides the conclusion of the paper.

\begin{table}
\centering
\caption{\label{tab:tab-comp-latex}Comparison of the key capabilities of the cossonet and cosso packages.}
\centering
\begin{tabular}[t]{l|l|l|l|l|l|l|l}
\hline
Response Type & cossonet & cosso & LASSO & Ridge & Elastic net & main effect & two way interaction\\
\hline
Gaussian & \$\textbackslash{}checkmark\$ & \$\textbackslash{}checkmark & \$\textbackslash{}checkmark & \$\textbackslash{}checkmark & \$\textbackslash{}checkmark\$ & \$\textbackslash{}checkmark\$ & \$\textbackslash{}checkmark\$\\
\hline
Binomial & \$\textbackslash{}checkmark\$ & \$\textbackslash{}checkmark\$ & \$\textbackslash{}checkmark\$ & \$\textbackslash{}checkmark\$ & \$\textbackslash{}checkmark\$ & \$\textbackslash{}checkmark\$ & \$\textbackslash{}times\$\\
\hline
Poisson & \$\textbackslash{}checkmark\$ & \$\textbackslash{}times\$ & \$\textbackslash{}times\$ & \$\textbackslash{}times\$ & \$\textbackslash{}times\$ & \$\textbackslash{}checkmark\$ & \$\textbackslash{}checkmark\$\\
\hline
Cox & \$\textbackslash{}checkmark\$ & \$\textbackslash{}checkmark\$ & \$\textbackslash{}checkmark\$ & \$\textbackslash{}checkmark\$ & \$\textbackslash{}checkmark\$ & \$\textbackslash{}checkmark\$ & \$\textbackslash{}times\$\\
\hline
\end{tabular}
\end{table}

\hypertarget{sec2}{%
\section{Model}\label{sec2}}

\hypertarget{ssec2_1}{%
\subsection{Smoothing Spline ANOVA}\label{ssec2_1}}

The functional ANOVA decomposition expresses a multivariate function \(f\) as a sum of components, capturing different levels of interactions among the explanatory variables. For explanatory variables \(x = (x^{(1)}, \dots, x^{(d)}) \in \mathcal{X} = [0, 1]^d\), the decomposition is represented as
\begin{align}
    f(x) = f_0 + \sum_{j=1}^{d} f_j(x^{(j)}) + \sum_{j < k} f_{jk}(x^{(j)}, x^{(k)}) + \cdots + f_{1, \dots, d}(x^{(1)}, \dots, x^{(d)}),
    \label{eq:decom} 
\end{align}
where \(f_0\) is a constant term, \(f_j\)'s are the main effects, and \(f_{jk}\)'s are two-way interactions, and so on. The identifiability of the terms in equation \eqref{eq:decom} is ensured by certain side conditions. To simplify the model, SS-ANOVA typically considers only lower-order interactions, such as main effects and two-way interactions.

Each main effect \(f_j(x^{(j)})\) belongs to a reproducing kernel Hilbert space (RKHS) \(\mathcal{H}^{(j)}\) which is decomposed as
\begin{align}
    \mathcal{H}^{(j)} = \mathcal{H}_0^{(j)} \oplus \mathcal{H}_1^{(j)},
    \label{eq:space} 
\end{align}
where \(\mathcal{H}_0^{(j)}\) is the mean space and \(\mathcal{H}_1^{(j)}\) is the contrast space. If \(x^{(j)}\) is continuous, \(\mathcal{H}^{(j)}\) is often considered as the second-order Sobolev Hilbert space
\begin{align*}
    W_2[0,1] = \{ g : g \text{ and } g' \text{ are absolutely continuous, and } g'' \in \mathcal{L}_2[0,1] \}.
\end{align*}
The inner product with respect to \(W_2[0,1]\) is defined as
\begin{align*}
    <f,g> = \left[ \int_{0}^{1} g(t) \, dt \right]^2 + \left[ \int_{0}^{1} g'(t) \, dt \right]^2 + \int_{0}^{1} [g''(t)]^2 \, dt,
\end{align*}
and the corresponding reproducing kernel is
\begin{align*}
    K(s,t) = 1 + k_1(s) k_1(t) + k_2(s) k_2(t) - k_4(|s-t|),
\end{align*}
where \(k_1(s) = s - 1/2\), \(k_2(s) = (k_1^2(s) - 1/12)/2\), and \(k_4(t) = (k_1^4(t) - k_1^2(t)/2 + 7/240)/24\). For derivation of the reproducing kernel for the continuous variable, see Chapter 2.3 of \citet{Gu:2013}.
Suppose that categorical variables \(x^{(j)}\) is taken \(L\) distinct values from the set \(\{1, \dots ,L\}\) and \(f_j(x^{(j)})\) is represented as an \(L\)-vector. The space \(\mathcal{H}_0^{(j)}\) is defined as \(\{f : f(1) = \cdots = f(L)\}\) and \(\mathcal{H}_1^{(j)}\) is defined as \(\{f : f(1) + \cdots + f(L) = 0\}\). The reproducing kernel for \(\mathcal{H}_1^{(j)}\) in equation \eqref{eq:space} is
\begin{align*}
    K_1(s,t) = L \cdot I(s = t) - 1,
\end{align*}
where \(I(\cdot)\) is the indicator function. For the derivation of the reproducing kernel for the category variable, see Chapter 2.2 of \citet{Gu:2013}.

The full space corresponding to the decomposition \eqref{eq:decom} is expressed as a tensor product
\begin{align}
    \otimes_{j=1}^{d} \mathcal{H}^{(j)} = \{1\} \oplus 
    \left[ \oplus_{j=1}^{d} \mathcal{H}_1^{(j)} \right] \oplus \left[ \oplus_{j<k} \mathcal{H}_1^{(j)} \otimes \mathcal{H}_1^{(k)} \right] \oplus \cdots.
    \label{eq:tensor} 
\end{align}
If \(x^{(j)}\) is continuous, the reproducing kernel of the tensor product space \(\mathcal{H}^{(j)} \otimes \mathcal{H}^{(k)}\) is the product of \(K(s^{(j)},t^{(j)})\) and \(K(s^{(k)},t^{(k)})\). If \(x^{(j)}\) is categorical, the reproducing kernel is then the product of \(K_1(s^{(j)},t^{(j)})\) and \(K_1(s^{(k)},t^{(k)})\). Thus, the full space \(\mathcal{F}\) corresponding to equation \eqref{eq:tensor} is
\begin{align}
    \mathcal{F} = \{1\} \oplus_{v=1}^{p} \mathcal{F}^{(v)},
    \label{eq:full-decom} 
\end{align}
where \(\mathcal{F}^{(1)}, \dots, \mathcal{F}^{(p)}\) are \(p\) orthogonal subspaces of \(\mathcal{F}\). For the main effects model, \(p = d\) and each \(\mathcal{F}^{(v)}\) corresponds to a subspace of the main effects. This case is equivalent to an additive model. For the two-way interactions model, \(p = d(d+1)/2\) and \(\mathcal{F}^{(v)}\) include subspaces of the main effects and the two-way interactions.

\hypertarget{ssec2_2}{%
\subsection{COSSO penalized likelihood}\label{ssec2_2}}

The COSSO models first derived under the Gaussian regression framework \citep{Lin:2006} and the exponential family \citep{Zhang:2006} and CoxPH regression model \citep{Leng:2006}. These models can be implemented using the \CRANpkg{cosso} package, which supports continuous, binary class, and survival responses. Our package extends the capabilities of the \CRANpkg{cosso} package to support non-negative count responses. Consider a regression problem where \(Y_i=f(x_i) + \epsilon_i, i = 1,\dots, n\) with \(x_i \in [0, 1]\) and \(\epsilon \sim N(0, \sigma^2)\), where \(Y\) represents the response value corresponding to \(x\). The COSSO penalized likelihood for estimating \(f\) is defined by
\begin{align}
    -\ell(f(x), Y) + \tau^2 J(f),
    \label{eq:cosso} 
\end{align}
where \(\ell\) is the log-likelihood function and \(\tau>0\) is the smoothing parameter, and \(J(f)\) is the roughness penalty.

The log-likelihood \(\ell\) is determined by the type of response variable. We first consider the exponential family. Assume that the response variable \(Y\) follows a distribution from the exponential family with the density
\begin{align*}
    \exp \left\{ \frac{y f(x) - B(f(x))}{A(\sigma)} + D(y, \sigma) \right\},
\end{align*}
where \(A > 0, B\) and \(D\) are known functions, and \(\sigma\) is a nuisance parameter. From the properties of the exponential family, the conditional mean of \(Y\) given \(X = x\) is \(\mathrm{E}(Y|x) = \dot{B}(f(x)) = \mu(x)\), and the conditional variance is \(\text{Var}(Y|x) = \ddot{B}(f(x)) A(\sigma) = \nu(x) A(\sigma)\), where \(\dot{B}\) and \(\ddot{B}\) denote the first and second derivatives of \(B\), respectively. The log-likelihood for the exponential family is given by
\begin{align}
    \ell(f(x), y) = \sum_{i=1}^{n} \left\{ y_i f(x_i) - B(f(x_i)) \right\}.
    \label{eq:loglik} 
\end{align}
The cases below describe the types of responses considered in our model.

\hypertarget{case-1.-gaussian-regression}{%
\subsubsection{Case 1. Gaussian regression}\label{case-1.-gaussian-regression}}

Assume that the response variables follow a Gaussian distribution, \(Y_i|x_i \sim N(f(x_i), \sigma^2)\). One has \(A(\sigma) = \sigma^2, B(f) = f^2/2\), and \(D(y, \sigma) = -y^2/(2\sigma^2)\).

\hypertarget{case-2.-logistic-regression}{%
\subsubsection{Case 2. Logistic regression}\label{case-2.-logistic-regression}}

Consider a binary response variable, where \(Y_i \in \{0, 1\}\) and the probability of \(Y_i=1\) given \(x_i\) is \(P(Y_i=1|x_i) = \pi(x_i)\), such that \(\log \pi(x_i)/ (1-\pi(x_i)) = f(x_i)\). One has \(A(\sigma) = 1, B(f) = \log(1 + \exp(f))\), and \(D(y, \sigma) = 1\).

\hypertarget{case-3.-poisson-regression}{%
\subsubsection{Case 3. Poisson regression}\label{case-3.-poisson-regression}}

For the nonnegative count response \(Y \in \{0, 1, 2, \dots \}\), assumed to follow a Poisson distribution with \(P(Y_i=y_i|x_i) = \lambda^{y_i} e^{-\lambda_i}/(y_i !)\), where \(\log \lambda_i = f(x_{i})\). One has \(A(\sigma) = 1, B(f) = \exp(f)\), and \(D(y, \sigma) = -\log(y!)\).

\hypertarget{case-4.-coxph-regression}{%
\subsubsection{Case 4. CoxPH regression}\label{case-4.-coxph-regression}}

We next consider of survival data analysis. Let the censored times be \(Y = \min \{T, C \}\), where \(T\) is the event time and \(C\) is the censoring time. Assume that \(T\) and \(C\) are conditionally independent given \(X = x\) and censoring mechanism is independent. Define \(\delta = I(T \leq C)\) as the event indicator. The observed data consists of the triplet \(\{Y_i, \delta_i, x_i\}_{i=1}^{n}\).

For simplicity, we assume that the observed failure times are distinct. When failure times are tied, the technique proposed by \citet{Breslow:1974} can be applied. Let \(t_1 < \cdots < t_N\) be the unique and ordered failure times. For the observed data, the risk set right before \(t_j\) is defined as
\begin{align*}
    R_j = \{ i \mid y_i \geq t_j \},
\end{align*}
where \(R_j\) includes all observations \(i\) still at risk at \(t_j\). The hazard function given \(x\) is
\begin{align*}
    h(t | x) = h_0(t) \exp(f(x)),
\end{align*}
where \(h_0\) is an unspecified baseline hazard function. The partial log-likelihood is then
\begin{align}
    \ell(f(x_i)) = \sum_{j=1}^{N} \delta_j \left[ f(x_j) - \log \left\{ \sum_{i \in R_j} \exp(f(x_i)) \right\} \right].
    \label{eq:partiallik} 
\end{align}

\hypertarget{sec3}{%
\section{Algorithm}\label{sec3}}

\hypertarget{ssec3_1}{%
\subsection{Equivalent formulation}\label{ssec3_1}}

\citet{Lin:2006} proved that the minimizer of equation \eqref{eq:cosso} lies on a finite-dimensional space of the RKHS \(\mathcal{F}^{(v)}\). To simplify the computation, equation \eqref{eq:cosso} is replaced by the following equivalent formulation. The minimizer of equation \eqref{eq:cosso} is the the standard COSSO solution obtained by solving
\begin{align}
    \begin{split}
        &\min_{f, \theta} -\ell(f(x), y) + \lambda_0 \sum_{v=1}^{p} \theta_v^{-1} \|P^{(v)} f\|^2, \\
        &\text{subject to } \sum_{v=1}^{p} \theta_v \leq M, \quad \theta_v \geq 0, \quad v = 1, \dots, p.
    \end{split}
    \label{eq:cosso-equvalent} 
\end{align}
Here, \(J(f) = \sum_{v=1}^{d} \theta_v^{-1} \|P^{(v)} f\|^2\), where \(P^{(v)} f\) is the orthogonal projection of \(f\) onto \(\mathcal{F}^{(v)}\) and \(\| \cdot \|\) denotes the RKHS norm associated with \(\mathcal{F}^{(v)}\). \(\lambda_0 > 0\) is a constant, and the smoothing parameter \(M\) controls sparsity by constraining the sum of \(\theta_v\)'s, which is also regulate the solution's roughness. Equation \eqref{eq:cosso-equvalent} is similar to the formulation of a smoothing spline with multiple smoothing parameters and an alternative penalty on the \(\theta\)'s.

The elastic-net penalized COSSO extends \(\theta\) in equation \eqref{eq:cosso-equvalent} as
\begin{align}
    \begin{split}
        &\min_{f, \theta} -\ell(f(x), y) + \lambda_0 \sum_{v=1}^{p} \theta_v^{-1} \|P^{(v)} f\|^2, \\
        &\text{subject to } \gamma \sum_{v=1}^{p} \theta_v + (1 - \gamma) \sum_{v=1}^{p} \theta_v^2 \leq M, \quad \theta_v \geq 0, \quad v = 1, \dots, p.
    \end{split}
    \label{eq:encosso1} 
\end{align}
Equation \eqref{eq:encosso1} is equivalently expressed as
\begin{align}
    \begin{split}
        &\min_{f, \theta} -\ell(f(x), y) + \lambda_0 \sum_{v=1}^{p} \theta_v^{-1} \|P^{(v)} f\|^2  + n\lambda \left( \gamma \sum_{v=1}^{p} \theta_v + (1 - \gamma) \sum_{v=1}^{p} \theta_v^2 \right), \\
        &\text{subject to } \theta_v \geq 0, \quad v = 1, \dots, p,
    \end{split}
    \label{eq:encosso2} 
\end{align}
where \(\lambda>0\) is a smoothing parameter and the mixing parameter \(0 \leq \gamma \leq 1\) controls the balance between the LASSO and ridge penalties. When \(\gamma = 1\), equations \eqref{eq:encosso1} and \eqref{eq:encosso2} reduce to the standard COSSO.

For fixed \(\lambda_0\) and \(\lambda\), the representation theorem for smoothing spline guarantees that the minimizer of equations \eqref{eq:encosso1} and \eqref{eq:encosso2} is \(f(x) = \sum_{i=1}^{n} c_i K(x_i, x) + b\), where \(c_i\) and \(b \in \mathbb{R}\) are the smoothing spline parameters and \(K(x_i, x) = \sum_{v=1}^{p} \theta_v K^{(v)}(x_i, x)\) with \(K^{(v)}(x_i, x)\) being the reproducing kernels in \(\mathcal{F}^{(v)}\). For simply notation, denote \(K^{(v)}\) as the \(n \times n\) matrix \(\{ K^{(v)}(x_i, x_k) \}_{i,k=1}^n\) and \(K\) as the matrix \(\{ K(x_i, x) \}_{i=1}^{n}\). Let \(\pmb{c} = (c_1, \dots, c_n)^T\) and \(\pmb{\theta} = (\theta_1, \dots, \theta_p)^T\). Following the standard COSSO framework, the roughness penalty in equations \eqref{eq:encosso1} and \eqref{eq:encosso2} is given by \(\sum_{v=1}^{d}\theta_v^{-1} \| {P^{(v)} f} \|^2 = \sum_{v=1}^{d} \theta_v \pmb{c}^T K^{(v)} \pmb{c} = \pmb{c}^T K \pmb{c}\). The matrix formulation of equation \eqref{eq:encosso2} becomes
\begin{align}
    \min_{b, \pmb{c}, \pmb{\theta} \geq \pmb{0}}
    -\frac{1}{n}\sum_{i=1}^{n}\ell(f(x_i), y_i)
    + \lambda_0 \pmb{c}^T K \pmb{c}
    + \lambda (\gamma \pmb{1}_d^T \pmb{\theta} + (1-\gamma) \pmb{\theta}^T \pmb{\theta}).
    \label{eq:glmcosso} 
\end{align}
This matrix formulation enables practical computation of equation \eqref{eq:encosso2}. Section @ref(ssec3\_2) outlines the coordinate descent algorithm for the exponential family, and Section @ref(ssec3\_3) addresses the algorithm for the CoxPH regression model.

\hypertarget{ssec3_2}{%
\subsection{Elastic-net based COSSO for exponential family}\label{ssec3_2}}

The basic approach to handling exponential families in the COSSO framework is to optimize \(f\) by applying Newton iterative method \citep{Gu:2013}. Our coordinate descent algorithm is derived from weighted least squares based on Newton iteration. Given the current solution \(f^0(x_i)\), the conditional mean and variance of \(Y\) given \(x_i\) are \(u_i^0 = \dot{B}(f^0(x_i))\) and \(V_i^0 = \ddot{B}(f^0(x_i))\), respectively. With \(\nu_i = -y_i + \mu_i^0\) and \(w_i = V_i^0\), the second-order Taylor expansion of \(-y_i f(x_i) + B(f(x_i))\) at \(f^0(x_i)\) is expressed as
\begin{align*}
    &-y_i f^0(x_i) + B(f^0(x_i)) + \nu_i\{f(x_i) - f^0(x_i)\}
    + \frac{1}{2} w_i \{f(x_i) - f^0(x_i)\}^2 \\
    &= \frac{1}{2} w_i \left\{f(x_i) - f^0(x_i) + \frac{\nu_i}{w_i}\right\}^2 + O_i,
\end{align*}
where \(O_i\) is independent of \(f(x_i)\). Let \(z_i = f^0(x_i) + (y_i - \mu_i^0)/w_i\) be the working response. For matrix formulation, let \(W = \text{diag}(w_1, \dots, w_n)\) and \(\pmb{z} = (z_1, \dots, z_n)^T\). The Newton iteration for solving \eqref{eq:glmcosso} is equivalent to the minimizer of the following objective function
\begin{align}
    \min_{b, \pmb{c}, \pmb{\theta} \geq \pmb{0}} 
    ( \pmb{z} - U \pmb{c} - b \pmb{1}_n )^T W ( \pmb{z} - U \pmb{c} - b \pmb{1}_n )
    + n \lambda_0 \pmb{c}^T U \pmb{c} 
    + \lambda (\gamma \pmb{1}_d^T \theta + (1-\gamma) \theta^T \theta).
    \label{eq:expcosso} 
\end{align}

\citet{Lin:2006} and \citet{Zhang:2006} proposed a two-step algorithm to solve equation \eqref{eq:expcosso}. In the first step, with \(\pmb{\theta}\) fixed, the solution for \((b, \pmb{c})\) is obtained as the solution of a smoothing spline. In the second step, with \((b, \pmb{c})\) fixed, the solution for \(\pmb{\theta}\) is obtained as the solution of a non-negative garrote. In the algorithms of \citet{Lin:2006} and \citet{Zhang:2006}, the inverse matrix is needed to solve these two steps. However, the \CRANpkg{cosso} package use \texttt{glmnet} and \texttt{solve.QP} for pratical purposes. When the \texttt{ginv} function is used to calculate the Moore-Penrose generalized inverse, the computational complexity is \(\mathcal{O}(n^3+p^3)\) per iteration.
\citet{Lin:2006} proposed an alternative algorithm to reduce computational complexity for large sample sizes \(n\) achieves \(\mathcal{O}(nm^2+p^3)\) per iteration. Since the subset is independent of \(p\), this approach still offers no improvement for \(p\). In contrast, our coordinate descent algorithm achieves efficiency for both \(n\) and \(p\) with a complexity of \(\mathcal{O}(nm^2+np)\) per iteration.

\hypertarget{coordinate-descent-algorithm}{%
\subsubsection{Coordinate Descent Algorithm}\label{coordinate-descent-algorithm}}

\citet{Lin:2006} showed that simulation performance of the subset-basis algorithm was comparable to that of the full-basis algorithm. However, the subset-based algorithm significantly improves computational efficiency compared to the full-basis algorithm. We solve equation \eqref{eq:expcosso} using a two-step subset-based approach as proposed by \citet{Lin:2006}.
Suppose that we have \(m \leq n\) subsets, then \(\pmb{c} = (c_1, \dots, c_m)^T\). Let \(Q\) be the matrix \(\{K(x_{k^*},x_{l^*})\}_{k,l=1}^{m}\) and \(Q^{(v)}\) be the matrix \(\{K^{(v)}(x_{k^*},x_{l^*})\}_{k,l=1}^{m}\). Let \(U\) be the matrix \(\{K(x_{k},x_{l^*})\}\) and \(U^{(v)}\) be the matrix \(\{K^{(v)}(x_{k},x_{l^*})\}\) for \(k=1,\dots, n\) and \(l=1,\dots, m\). Let \(U_{ij}\) and \(Q_{ij}\) denote the \((i,j)\)th entries of \(U\) and \(Q\), respectively. The solution to the subspace is approximated by \(f(x) = \sum_{i=1}^{m} c_i \sum_{v=1}^{p} \theta_v U^{(v)} + b\). Equation \eqref{eq:expcosso} becomes
\begin{align}
    \min_{b, \pmb{c}, \pmb{\theta} \geq \pmb{0}}
    \sum_{i=1}^{n}w_i \left( z_i - \sum_{k=1}^{m} c_k U_{ik} - b \right)^2 
    + n \lambda_0 \sum_{k=1}^{m} \sum_{l=1}^{m}c_k c_l Q_{kl}
    + \lambda \left(\gamma \sum_{v=1}^{p} \theta_v + (1-\gamma) \sum_{v=1}^{p} \theta_v^2 \right).
    \label{eq:expcosso-algo} 
\end{align}
Following \citet{Tibshirani:1996}, equation \eqref{eq:expcosso-algo} can be transformed into a penalized weighted least squares (WLS) problem. Solving for the WLS solution is efficient for existing algorithm by avoiding matrix inversion. Our algorithm finds the WLS solution at each step. When \(\pmb{\theta}\) is fixed, equation \eqref{eq:expcosso-algo} simplifies to
\begin{align}
    \min_{b, \pmb{c}} \sum_{i=1}^{n}w_i \left( z_i - \sum_{k=1}^{m} c_k U_{ik} - b \right)^2
    + n \lambda_0 \sum_{k=1}^{m} \sum_{l=1}^{m}c_k c_l Q_{kl}.
    \label{eq:expcosso-cstep-algo} 
\end{align}
This is equivalent to a weighted smoothing spline. The solutions for \(b\) and \(\pmb{c}\) is given by
\begin{align}
    \begin{split}
        \hat{c}_j &= \frac{2\sum_{i=1}^{n}w_i U_{ij}(z_i-\sum_{l \neq j}U_{il}c_{l}-b) - n\lambda_0 \sum_{l \neq j}^{m}Q_{jl}c_{l}}
                          {2\sum_{i=1}^{n}w_i U_{ij}^2 + n\lambda_0 Q_{jj}}, \quad j = 1,\dots, m, \\
        \hat{b} &= \frac{\sum_{i=1}^{n}w_i (z_i-\sum_{l=1}^{n}U_{il}c_{l})}{\sum_{i=1}^{n}w_i}.
    \end{split}
    \label{eq:expcosso-cstep-sol} 
\end{align}
When \((b, \pmb{c})\) is fixed, let \(u_i=\sqrt{w}_i(z_i-b)\), \(G\) be the \(n \times p\) matrix where the \(i\)th row and \(v\)th column element is \(\sqrt{w}_i \sum_{k=1}^{m}U_{ik}^{(v)}c_k\), and \(h_v\) be the vector whose \(v\)th element is \(n\lambda_0 \sum_{k=1}^{m}\sum_{l=1}^{m}c_k c_l Q_{kl}^{(v)}\) for \(v = 1, \dots, d\). With this, equation \eqref{eq:expcosso-algo} becomes
\begin{align}
    \begin{split}
        &\min_{\pmb{\theta} \geq \pmb{0}} 
        \sum_{i=1}^{n} \left( u_i - \sum_{v=1}^{p} \theta_v G_{iv} \right)^2
        + \sum_{v=1}^{p} h_v \theta_v 
        + \lambda \left(\gamma \sum_{v=1}^{p} \theta_v + (1-\gamma) \sum_{v=1}^{p} \theta_v^2 \right).
    \end{split}
    \label{eq:expcosso-thetastep-algo} 
\end{align}
This is equivalent to a non-negative garrote with elastic-net regularization. The solution for \(\pmb{\theta}\) is
\begin{align}
    \hat{\theta}_j 
    = \frac{\mathcal{S}\left(\sum_{i=1}^{n} G_{ij}(u_{i}-\sum_{k \neq j} G_{ij} \theta_{j}) - h_{j}/2, n\lambda \gamma/2 \right)}
           {\sum_{i=1}^{n} G_{ij}^2 + n\lambda (1-\gamma)}, \quad \text{for } j = 1, \dots, p,
  \label{eq:expcosso-thetastep-sol} 
\end{align}
where \(\mathcal{S}(\alpha, \beta)\) is the soft-thresholding operator defined as
\begin{align*}
    \mathcal{S}(\alpha, \beta) 
    &= \text{sign}(\alpha)\left[|\alpha| - \beta\right]_+ \\
    &= \begin{cases}
        \alpha - \beta, & \text{if } \alpha > 0 \text{ and } \beta < |\alpha|, \\
        0, & \text{otherwise.}
    \end{cases} 
\end{align*}
Given a new data point \(x^* = (x_1^*, \dots, x_p^*)\), the function is predicted by
\begin{align*}
    \hat{f}(x^*) = \hat{b} + \sum_{i=1}^{m} \hat{c}_i \sum_{v=1}^{p} \hat{\theta}_v U^{(v)}(x^*, x_i^*).
\end{align*}
This predicted value outcomes for new data points based on the fitted model. To obtain accurate predictions, it is important to choose an appropriate smoothing parameter, which is discussed in the following sections.

\hypertarget{smoothing-parameter-selection}{%
\subsubsection{Smoothing parameter selection}\label{smoothing-parameter-selection}}

The smoothing parameters \(\lambda_0\) and \(\lambda_\theta\) control the trade-off between the likelihood function and the penalty term. The problem of smoothing parameter selection is directly related to selecting the optimal model. \(\lambda_0\) and \(\lambda_\theta\) control the strength of the regularization. A higher \(\lambda_\theta\) value results in more coefficient shrinkage and sparsity. In order for theta to be chosen correctly, both \(\lambda_0\) and \(\lambda_\theta\) must be set to a grid with an appropriate range. We choose \(\lambda_0\) and \(\lambda_\theta\) by cross-validation (CV) at each step.

\citet{Gu:2013} discussed performance-oriented iteration and CV as criterias for smoothing parameter selection. For CV, \citet{Zhang:2006} adopted Kullbeck-Leibler (KL) divergence which is one of the measurement for performance-oriented iteration. KL divergence has the advantage of directly evaluating the model with observed data without additional matrix calculation. \citet{Lin:2006} adopted generalized cross-validation (GCV) in Gaussian regression framework. We adopted GCV defined as
\begin{align*}
    \text{GCV} = \frac{n\sum_{i=1}^{n}w_i(y_i-\hat{f}_i)^2}{\{n-tr(H)\}^2}.
\end{align*}
It is straightforward to derive and easy to apply within the WLS framework. To compute this, we need the degrees of freedom \(\text{tr}(H)\), which is obtained as the sum of the diagonal elements of the hat matrix \(H\), where \(h_{ii} = U_i (U^T U + \lambda_0 Q)^{-1} U_i^T\).
The complete algorithm that combining one-step update and parameter tuning is summarized below.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Initialize \(f_i = f(x_i) = \bar{y}, \mu_i = \dot{B}(f_i), w_i = \ddot{B}(f_i)\), and \(z_i = f_i - (y_i - \mu_i)/w_i\) for \(i = 1, \dots, n\). Set \(\theta_v = 1\) for \(v = 1, \dots, p\), and define grids for \(\lambda_0\) and \(\lambda\).
\item
  With fixed \(\hat{\theta}_1, \dots, \hat{\theta}_p\), compute \(z_1, \dots, z_n,U\) and \(Q\). Solve for \((\hat{b}, \hat{\pmb{c}})\) using \eqref{eq:expcosso-cstep-sol}, and calculate the GCV across all \(\lambda_0\) grid points. Select the optimal \(\lambda_0\) and corresponding \((\hat{b}, \hat{\pmb{c}})\) that minimize the average GCV.
\item
  With fixed \((\hat{b}, \hat{\pmb{c}})\), compute \(h_1, \dots, h_p\) and \(G\). Solve for \(\hat{\theta}_1, \dots, \hat{\theta}_p\) using \eqref{eq:expcosso-thetastep-sol}, and calculate the GCV across all \(\lambda\) grid points. Select the optimal \(\lambda\) and corresponding \(\hat{\theta}_1, \dots, \hat{\theta}_p\) that minimize the average GCV.
\item
  Repeat Steps 2 and 3 until convergence.
\end{enumerate}

The 1-standard error rule can be applied to obtain a simpler model than the one derived using the above procedure. The \pkg{cossonet} package offers options to implement this rule.

\hypertarget{ssec3_3}{%
\subsection{Elastic-net based COSSO for CoxPH regression model}\label{ssec3_3}}

\citet{Leng:2006} proposed an objective function for the CoxPH regression model within the COSSO framework. We extend this model by applying an elastic-net penalty, as defined in the following objective function
\begin{align}
    \min_{\pmb{c}, \pmb{\theta} \geq \pmb{0}}
    -\frac{1}{n}\pmb{\delta}^T U\pmb{c} + \frac{1}{n} \sum_{j=1}^{N} \log \left( \sum_{i \in R_j} e^{U_i \pmb{c}} \right) 
    + \lambda_0 \pmb{c}^T Q \pmb{c} 
    + \lambda (\gamma \pmb{1}_p^T \pmb{\theta} + (1-\gamma) \pmb{\theta}^T \pmb{\theta}),
    \label{eq:coxcosso} 
\end{align}
where \(\pmb{\delta} = (\delta_1, \dots, \delta_n)\) is the vector of censoring indicators and \(U_i\) is the \(i\)th row of \(U\), and the remaining matrix notation follows the definitions provided in Section @ref\{ssec3\_1\}.

\citet{Leng:2006} derived a subset-based two-step algorithm where \(\pmb{c}\) and \(\pmb{\theta}\) are updated iteratively while keeping the other fixed. To solve \(\pmb{c}\) and \(\pmb{\theta}\), the gradient and Hessian of both are calculated. The \(\pmb{c}\) vector is updated using the Newton-Raphson iteration, while \(\pmb{\theta}\) is solved via quadratic programming with linear constraints. Since it is based on matrix inversion, the subset-based approach has a computational complexity of \(\mathcal{O}(nm^2 + p^3)\). In practice, the \pkg{cosso} package employs \texttt{glmnet} and \texttt{solve.QP} to estimate \(\pmb{c}\) and \(\pmb{\theta}\), respectively. we extend the subset-based coordinate descent approach using WLS \citet{Simon:2011} to the CoxPH regression model. The computation complexity becomes \(\mathcal{O}(nm+np)\) per iteration.

\hypertarget{coordinate-descent-algorithm-1}{%
\subsubsection{Coordinate Descent Algorithm}\label{coordinate-descent-algorithm-1}}

We rewrite equation \eqref{eq:coxcosso} as the observed objective function
\begin{align}
    \min_{\pmb{c}, \pmb{\theta} \geq \pmb{0}}
    -\frac{1}{n}\sum_{j=1}^{N} \delta_j \left( 
    U_j\pmb{c} + \log \left( \sum_{i \in R_j} e^{U_i \pmb{c}} \right) \right) 
    + n\lambda_0 \sum_{k=1}^{m} \sum_{l=1}^{m}c_k c_l Q_{kl}
    + n\lambda \left(\gamma \sum_{v=1}^{p} \theta_v + (1-\gamma) \sum_{v=1}^{p} \theta_v^2 \right),
    \label{eq:coxcosso-obs} 
\end{align}
where the first term sums over the non-censored rows of \(U\).
Following \citet{Simon:2011}, equation \eqref{eq:coxcosso-obs} can be transformed into a WLS formulation. Solving for the WLS solution is more efficient than existing algorithm by avoiding direct calculation the gradient and Hessian for \(\pmb{c}\) and \(\pmb{\theta}\). Our algorithm easily finds a solution as the WLS solution at each step. For simplicity of calculation, assume that the intercept is absorbed through data scaling. When \(\pmb{\theta}\) is fixed, equation \eqref{eq:coxcosso-obs} reduces to \eqref{eq:expcosso-cstep-algo} where the weight and working response are given by
\begin{align*}
    \begin{split}
        w_i &= \sum_{j=1}^{N} \left[ \frac{\sum_{i \in R_j}U_i^T U_i e^{U_i \pmb{c}}}{\sum_{i \in R_j} e^{U_i \pmb{c}}}
        - \frac{\sum_{i \in R_j}U_i^T e^{U_i \pmb{c}}}{\sum_{i \in R_j} e^{U_i \pmb{c}}} 
        \frac{\sum_{i \in R_j}U_i^T e^{U_i \pmb{c}}}{\sum_{i \in R_j} e^{U_i \pmb{c}}} \right], \\
        z_i &= f_i + \frac{1}{w_i} \left[ \delta_i + \sum_{j=1}^{N} \frac{\sum_{i \in R_j}U_i^T e^{U_i \pmb{c}}}{\sum_{i \in R_j} e^{U_i \pmb{c}}} \right],
    \end{split}
\end{align*}
respectively. These can be easily computed using the XX function in \texttt{glmnet}. The weight \(w_i\) in \CRANpkg{cosso} relates to the Hessian of \(\pmb{c}\), which involves \(n^2\) calculation and is computationally intensive. However, our algorithm only needs \(n\) calculations. The solution for \(\pmb{c}\) is equivalent to \eqref{eq:expcosso-cstep-sol} without intercept. When \((b, \pmb{c})\) is fixed, let \(u_i=\sqrt{w}_i z_i\), \(G\) be the \(n \times p\) matrix where the \(i\)th row and \(v\)th column element is \(\sqrt{w}_i \sum_{k=1}^{m}U_{ik}^{(v)}c_k\), and \(h_v\) be the vector whose \(v\)th element is \((n/\lambda_0) \sum_{k=1}^{m}\sum_{l=1}^{m}c_k c_l Q_{kl}^{(v)}\) for \(v = 1, \dots, d\). With this, equation \eqref{eq:coxcosso-obs} becomes \eqref{eq:expcosso-thetastep-algo}. This is equivalent to the non-negative garrote with elastic-net regularization. The solution for \(\pmb{\theta}\) is equivalent to equation \eqref{eq:expcosso-thetastep-sol}.
Given a new data point \(x^* = (x_1^, \dots, x_p^*)\), the predicted function is obtained by
\begin{align*}
    \hat{f}(x^*) = \sum_{i=1}^{m} \hat{c}_i \sum_{v=1}^{p} \hat{\theta}_v U^{(v)}(x^*, x_i^*).
\end{align*}

In the CoxPH regression model, \citet{Leng:2006} used approximate cross-validation based on KL divergence for parameter tuning. However, this measure is evaluated by the Hessian matrix obtained during the update, which is not required in our algorithm. Instead, we apply GCV within the WLS framework, following the full algorithm described in Section @ref(ssec3\_2).

\hypertarget{sec4}{%
\section{Example usage}\label{sec4}}

\pkg{cossonet} is available from the Comprehensive R Archive Network. The main algorithm of the \pkg{cossonet} package is written in \texttt{C} and calls \texttt{C} code from \texttt{R} using \texttt{.Call()}. The main function \texttt{cossonet} accepts all of the response types we consider and returns an object of the S3 class \texttt{cossonet}. This section provides examples of how to use the \pkg{cossonet} package. We first load the library for \pkg{cossonet} and set a seed for reproducibility.

\begin{verbatim}
devtools::install_github("jiieunshin/cossonet")
library(cossonet)
set.seed(20250101)
\end{verbatim}

\hypertarget{ssec4_1}{%
\subsection{Data generation}\label{ssec4_1}}

The function \texttt{data\_generation} generates example datasets with response specified as continuous, binary, non-negative count, or survival data. The detailed process of generating datasets is explained in Section @ref\{sec5\}. We generate a training set with \(n=200\) and \(p=20\), and a test set with \(n=200\) and \(p=20\).

\begin{verbatim}
tr = data_generation(n = 200, p = 20, SNR = 8, response = "regression",)
str(tr)
\end{verbatim}

\begin{verbatim}
#> List of 3
#>  $ x: num [1:200, 1:20] 0.377 0.579 0.239 0.572 0.877 ...
#>  $ f: num [1:200] 2.9493 0.0907 5.6039 -0.8799 0.331 ...
#>  $ y: num [1:200] 2.9493 0.0907 5.6039 -0.8799 0.331 ...
\end{verbatim}

\begin{verbatim}
te = data_generation(n = 1000, p = 20, SNR = 8, response = "regression")
str(te)
\end{verbatim}

\begin{verbatim}
#> List of 3
#>  $ x: num [1:1000, 1:20] 0.873 0.619 0.416 0.431 0.205 ...
#>  $ f: num [1:1000] -0.0916 0.2171 -1.5693 -1.0491 4.1775 ...
#>  $ y: num [1:1000] -0.0916 0.2171 -1.5693 -1.0491 4.1775 ...
\end{verbatim}

\hypertarget{ssec4_2}{%
\subsection{4.2 Model fitting}\label{ssec4_2}}

The function \texttt{cossonet} takes an \(n \times p\) matrix and a response vector as input to fit the model. The \texttt{family} argument specifies the response type, defaulting to \texttt{gaussian} if not provided. It also includes arguments for model fitting, such as the kernel function and grids for smoothing parameters. If not specified, default values are used. See Table \ref{tab:tab-function-latex} for details on the arguments. We demonstrate \texttt{cossonet} with the simplest settings.

\begin{verbatim}
fit = cossonet(tr$x, tr$y, family = "gaussian", 
              lambda0 = exp(seq(log(2^{-6}), log(2^{-3}), length.out = 20)),
              lambda_theta = exp(seq(log(2^{-8}), log(2^{-4}), length.out = 20))
              )
\end{verbatim}

\begin{verbatim}
#> fit COSSO  with n =  200 p = 20 
#> kernel: spline and d = 20 
#> -- c-step -- 
#> proceeding...
\end{verbatim}

\begin{verbatim}
#> mse: 2.3582 
#> 
#> -- theta-step -- 
#> proceeding...
\end{verbatim}

\begin{figure}
\centering
\includegraphics{cossonet_files/figure-latex/model-fitting-1.pdf}
\caption{\label{fig:model-fitting}The five-fold CV plot for \(\lambda_0\) and \(\lambda\) from the \texttt{cossonet} run is shown. The horizontal axis represents the log of \(\lambda\), and the vertical axis represents the GCV. The red points are the average GCV for the validation set, while the vertical solid lines are the standard errors. If the 1-standard error rule is applied, the smoothing parameter is selected as the value corresponding to the dotted line in the current figure.}
\end{figure}

\begin{verbatim}
#> mse: 2.5687 
#> 
#> -- c-step -- 
#> proceeding... 
#> mse: 2.3582
\end{verbatim}

\begin{table}
\centering
\caption{\label{tab:tab-function-latex}Summary of the arguments of functions in `cossonet`.}
\centering
\begin{tabular}[t]{l|l}
\hline
Argument & Description\\
\hline
x & Input matrix of size \$n\$ by \$p\$, where each row represents an observation. It can be a matrix or data frame. \textbackslash{}texttt\{x\} must have at least two columns (\$p>1\$).\\
\hline
y & The response variable. If \textbackslash{}texttt\{family="gaussian"\} or \textbackslash{}texttt\{family="poisson"\} (non-negative counts), it is quantitative. If \textbackslash{}texttt\{family="binomial"\}, it must be a vector with two levels. If \textbackslash{}texttt\{family="cox"\}, \textbackslash{}texttt\{y\} must be a two-column matrix (or data frame) with columns named 'time' and 'state'.\\
\hline
family & The type of the response variable.\\
\hline
wt & The weights of the predictors. The default is \textbackslash{}texttt\{rep(1, ncol(x))\}.\\
\hline
scale & If \textbackslash{}texttt\{TRUE\}, continuous predictors are rescaled to the interval \$[0, 1]\$. The default is \textbackslash{}texttt\{TRUE\}.\\
\hline
cv & A measurement for cross-validation.\\
\hline
nbasis & The number of "knots" to choose from. If \textbackslash{}texttt\{basis.id\} is provided, it is ignored.\\
\hline
basis.id & An index that specifies the selected "knot".\\
\hline
kernel & The kernel function. Four types are provided: \textbackslash{}texttt\{linear\}, \textbackslash{}texttt\{gaussian\}, \textbackslash{}texttt\{poly\}, and \textbackslash{}texttt\{spline\} (default). - Linear kernel: \$K(x, y) = \textbackslash{}langle x, y \textbackslash{}rangle\$ - Gaussian kernel: \$K(x, y) = \textbackslash{}exp(-\textbackslash{}kappa \textbackslash{}langle x, y \textbackslash{}rangle)\$ - Polynomial kernel: \$K(x, y) = (\textbackslash{}langle x, y \textbackslash{}rangle +1)\textasciicircum{}\{\textbackslash{}kappa\}\$ - Spline kernel: \$K(x, y) = U(x, y)\$\\
\hline
effect & The effect of the component. \textbackslash{}texttt\{main\} (default) for the main effect, \textbackslash{}texttt\{interaction\} for two-way interactions.\\
\hline
kparam & Parameter \$\textbackslash{}kappa\$ for the kernel function. Used by Gaussian and polynomial kernels.\\
\hline
lambda0 & A vector of \$\textbackslash{}lambda\_0\$ sequences. The default is a grid of 20 values \$\textbackslash{}mathtt\{[2\textasciicircum{}\{-10\}, \textbackslash{}dots, 2\textasciicircum{}\{10\}]\}\$ on an equally spaced logarithmic scale. This may need to be adjusted based on the input data. Do not provide a single value for \$\textbackslash{}lambda\_0\$.\\
\hline
lambda & A vector of \$\textbackslash{}lambda\$ sequences. The default is a grid of 20 values \$\textbackslash{}mathtt\{[2\textasciicircum{}\{-10\}, \textbackslash{}dots, 2\textasciicircum{}\{10\}]\}\$ on an equally spaced logarithmic scale. This may need to be adjusted based on the input data. Do not provide a single value for \$\textbackslash{}lambda\$.\\
\hline
gamma & Elastic mesh mixing parameter, \$0 \textbackslash{}leq \textbackslash{}gamma \textbackslash{}leq 1\$. When \textbackslash{}texttt\{gamma=1\}, it uses LASSO penalty, and when \textbackslash{}texttt\{gamma=0\}, it uses ridge penalty. The default is \textbackslash{}texttt\{gamma=0.95\}.\\
\hline
\end{tabular}
\end{table}

Running the code above displays the algorithm's progress and the error for validation set at current step in the console. Figure \ref{fig:model-fitting} presents the GCV curve for the validation error during 5-fold CV at each step. The red dot is the average GCV and the vertical solid line represents the standard error.

\begin{verbatim}
str(fit)
\end{verbatim}

\begin{verbatim}
#> List of 5
#>  $ data      :List of 7
#>   ..$ x       : num [1:200, 1:20] 0.345 0.568 0.192 0.56 0.897 ...
#>   ..$ y       : num [1:200] 2.9493 0.0907 5.6039 -0.8799 0.331 ...
#>   ..$ Uv      : num [1:200, 1:40, 1:20] 0.02699 -0.00961 0.04974 -0.00836 -0.06566 ...
#>   ..$ basis.id: int [1:40] 10 16 17 19 33 34 35 36 43 45 ...
#>   ..$ wt      : num [1:20] 1 1 1 1 1 1 1 1 1 1 ...
#>   ..$ kernel  : chr "spline"
#>   ..$ kparam  : num 1
#>  $ tune      :List of 3
#>   ..$ lambda0     : num [1:20] 0.0156 0.0174 0.0194 0.0217 0.0242 ...
#>   ..$ lambda_theta: num [1:20] 0.00391 0.00452 0.00523 0.00605 0.007 ...
#>   ..$ gamma       : num 0.95
#>  $ c_step    :List of 12
#>   ..$ measure  : num [1:5, 1:20] 102.3 72.4 115.2 112.9 175.2 ...
#>   ..$ Uv       : num [1:200, 1:40, 1:20] 0.02699 -0.00961 0.04974 -0.00836 -0.06566 ...
#>   ..$ Q        : num [1:40, 1:40] 1.7046 0.2071 -0.0312 0.1282 0.0832 ...
#>   ..$ w.new    : num [1:200] 1 1 1 1 1 1 1 1 1 1 ...
#>   ..$ sw.new   : num [1:200] 1 1 1 1 1 1 1 1 1 1 ...
#>   ..$ mu.new   : num [1:200] 1.154 0.322 2.98 0.293 -0.953 ...
#>   ..$ z.new    : num [1:200] 2.9493 0.0907 5.6039 -0.8799 0.331 ...
#>   ..$ zw.new   : num [1:200] 2.9493 0.0907 5.6039 -0.8799 0.331 ...
#>   ..$ b.new    : num 0.963
#>   ..$ c.new    : num [1:40] 0.448 1.387 -1 -0.783 0.247 ...
#>   ..$ optlambda: num 0.125
#>   ..$ conv     : logi TRUE
#>  $ theta_step:List of 4
#>   ..$ cv_error       : num [1:5, 1:20] 62.3 46.6 50.9 49.3 36.3 ...
#>   ..$ optlambda_theta: num 0.0625
#>   ..$ gamma          : num 0.95
#>   ..$ theta.new      : num [1:20] 0 0.442 1.437 1.158 0 ...
#>  $ family    : chr "gaussian"
#>  - attr(*, "class")= chr "cdcosso"
\end{verbatim}

The fitted model includes the data used for model fitting and the estimated parameters at each step. The \texttt{data} list shows the training set input to \texttt{cossonet} and related information. The \texttt{tune} list displays the values for the tuning parameters. The \texttt{c\_step} list shows the outputs related for solving \((b, \pmb{c})\), and the \texttt{theta\_step} list shows the outputs related for solving \(\pmb{\theta}\). The \texttt{family} list shows the inputted response argument. The \texttt{theta.new} in the \texttt{theta\_step} list is the estimated component. If \texttt{theta.new} is greater than 0, it is considered significant component. The example below shows that the first four components are significant while the remaining components are not.

\begin{verbatim}
fit$theta_step$theta.new
\end{verbatim}

\begin{verbatim}
#>  [1] 0.000000 0.441998 1.437467 1.157956 0.000000 0.000000 0.000000 0.000000
#>  [9] 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000
#> [17] 0.000000 0.000000 0.000000 0.000000
\end{verbatim}

\hypertarget{ssec4_3}{%
\subsection{Prediction}\label{ssec4_3}}

The function \texttt{cossonet.predict} provides the predicted values for new data using the fitted model. The output of the function \texttt{cossonet.predict} includes the predicted values \(\hat{f}\) from \texttt{f.new} and \(\hat{\mu}\) from \texttt{mu.new} for the new data. The predicted values for the test set using our fitted model are as follows.

\begin{verbatim}
pred = cossonet.predict(fit, te$x)
str(pred)
\end{verbatim}

\begin{verbatim}
#> List of 2
#>  $ f.new : num [1:1000] -0.343 1.06 0.149 0.945 2.702 ...
#>  $ mu.new: num [1:1000] -0.343 1.06 0.149 0.945 2.702 ...
\end{verbatim}

In the gaussian family, since \texttt{f.new} represents the response variable, the mean squared error for evaluating test set can be calculated as follows.

\begin{verbatim}
mean((te$y - pred$f.new)^2)
\end{verbatim}

\begin{verbatim}
#> [1] 3.120764
\end{verbatim}

\hypertarget{sec5}{%
\section{Simulation}\label{sec5}}

In this section, we simulate the prediction and component selection performance of \pkg{cossonet} for all response types considered in this paper. The simulation data generation follows the methodology from \citet{Zhang:2006}, \citet{Lin:2006}, and \citet{Leng:2006}. For simulation, we call the function `data\_generation to generate simulation data in \pkg{cossonet}.

\hypertarget{ssec5.1}{%
\subsection{Simulation settings}\label{ssec5.1}}

Let us generate the \(p\) explanatory variables \(X = (X_1, \dots, X_p)\) in the range of \([0,1]\). The first four \(X_1, \dots, X_4\) are informative variables, which are generated from the structure \(X_j = (W_j + tV) / (1 + t)\), where \(W_1, \dots, W_p\) and \(V\) are i.i.d. variables generated from \(\text{Uniform}(0, 1)\). The informative variables \(X_j\) and \(X_k, j \neq k\) are correlated as \(\text{Cor}(X_j,X_k)=t^2/(1+t^2)\). When \(t=0\), there are no correlation. The remaining \(X_5, \dots, X_p\) are i.i.d and follow \(\text{Uniform}(0, 1)\). The true function \(f\) is nonlinear function constructed by the informative variables. The true function is generated slightly differently for each response type. Details are described below.

\hypertarget{case-1-continuous-response}{%
\subsubsection{Case 1: Continuous response}\label{case-1-continuous-response}}

The continuous response is generated according to the procedure outlined in \citet{Zhang:2006}. Consider the following nonlinear functions
\begin{align*}
    f_1(t) &= t \\
    f_2(t) &= (2t - 1)^2, \\
    f_3(t) &= \frac{\sin(2 \pi t)}{2 - \sin(2 \pi t)}, \\
    f_4(t) &= 0.1\sin(2 \pi t) + 0.2\cos(2 \pi t) + 0.3\sin(2 \pi t)^2 + 0.4\cos(2 \pi t)^3 + 0.5\sin(2 \pi t)^3.
\end{align*}
The forms of these nonlinear functions are shown in Figure \ref{fig:fig-nonlinear}. The true function for the continuous response is given by
\begin{align}
    f(x) =&  f_1(x_1) + f_2(x_2) + 2 f_3(x_3) + 3 f_4(x_4) + \varepsilon,
    \label{eq:truef}
\end{align}
where the error term \(\varepsilon\) is generated from a normal distribution \(N(0, \sigma)\) and \(\sigma\) is chosen to have a signal-to-noise ratio of 8:1. The smoothing parameters \(\lambda_0\) and \(\lambda\) are considered grids of 20 values \([2^{-5}, \dots, 2^{0}]\) and \([2^{-10}, \dots, 2^{-5}]\) on an equally spaced logarithmic scale, respectively.

\begin{figure}
\includegraphics[width=52.44in]{figures/nonlinear_funcs} \caption{Nonlinear functions for generating simulation data.}\label{fig:fig-nonlinear}
\end{figure}

\hypertarget{case-2-binary-response}{%
\subsubsection{Case 2: Binary response}\label{case-2-binary-response}}

The true nonlinear logit function for classification, as proposed by \citet{Lin:2006}, is given by
\begin{align*}
    f(x) &= 3x_1 + \pi \sin(\pi x_2) + 8 x_3^3 + \frac{2}{e - 1} e^{x_4} + \varepsilon.
\end{align*}
The logit form is given by \(\log \frac{\pi(x)}{1-\pi(x)}=f(x)\), which generates a binary class of 0 or 1, with the probability \(\pi(x)=P(X=1)\) in Bernoulli trials. The error \(\varepsilon\) follows \(N(0, \sigma)\), where \(\sigma\) is chosen to have a signal-to-noise ratio of 8:1. For the smoothing parameters \(\lambda_0\) and \(\lambda\), we consider grids of 20 values \([2^{-6}, \dots, 2^{-4}]\) and \([2^{-8}, \dots, 2^{-4}]\) on an equally spaced logarithmic scale, respectively.

\hypertarget{case-3-count-response}{%
\subsubsection{Case 3: Count response}\label{case-3-count-response}}

The count response is generated from a Poisson distribution with mean \(\mu=\exp(f)\). The true function \(f\) is generated using equation \eqref{eq:truef}. To ensure the true function remains positive, appropriate values are added to the nonlinear functions: \(f_5(t) = f_3(t) + 0.5\) and \(f_6(t) = f_3(t) + 0.5\). The true function is then given by
\begin{align*}
    f(x) =&  \frac{1}{3}(f_1(x_1) + f_2(x_2) + 2 f_5(x_3) + 3 f_6(x_4) + \varepsilon),
\end{align*}
where \(\varepsilon \sim N(0, \sigma)\), where \(\sigma\) is chosen to have a signal-to-noise ratio of 8:1. The smoothing parameters \(\lambda_0\) and \(\lambda\) are considered as grids of 20 values \([2^{-5}, \dots, 2^{0}]\) and \([2^{-8}, \dots, 2^{-4}]\) on an equally spaced logarithmic scale, respectively.

\hypertarget{case-4.-survival-response}{%
\subsubsection{Case 4. Survival response}\label{case-4.-survival-response}}

The survival response consists of set of time and state, where the experimental design follows the approach outlined by \citet{Leng:2006}. Based on the true function in equation \eqref{eq:truef}, the survival time \(T\) and the truncation time \(C\) are generated from exponential distributions with means \(\exp(f(x))\) and \(V \exp(-f(x))\), where \(V \sim \text{Uniform}(a, a+2)\). The parameter \(a\) controls the truncation fraction. For example, \(a=2\) applies 20\% truncation. The time response is given by \(\min\{T_i, C_i\}\) and the state is defined as \(I(T_i < C_i)\). The error term \(\varepsilon\) is generated from \(N(0, \sigma)\) with \(\sigma\) chosen to have a signal-to-noise ratio of 8:1. The smoothing parameters \(\lambda_0\) and \(\lambda\) are considered grids of 20 values \([2^{-5}, \dots, 2^{-2}]\) and \([2^{-10}, \dots, 2^{-8}]\) on an equally spaced logarithmic scale, respectively.

\begin{table}
\centering
\caption{\label{tab:simulate-table}Simulated results of continuous response for TP, FP, F1-score, and MSE for `cossonet` at  = 1 and 0.95, with standard errors in parentheses.}
\centering
\begin{tabular}[t]{r|r|>{}r|>{}r|>{}r|>{}r|>{}r|>{}r|>{}r|>{}r|r}
\hline
n & p & gamma & TP & FP & F1\_score & MSE & SE\_TP & SE\_FP & SE\_F1 & SE\_MSE\\
\hline
100 & 20 & \textbf{0.95} & \textbf{3.25} & \textbf{0.40} & \textbf{0.8449} & \textbf{2.6977} & \textbf{0.0757} & \textbf{0.0865} & \textbf{0.0143} & 0.0293\\
\hline
100 & 40 & \textbf{0.95} & \textbf{3.08} & \textbf{0.35} & \textbf{0.8207} & \textbf{2.6509} & \textbf{0.0800} & \textbf{0.0796} & \textbf{0.0139} & 0.0282\\
\hline
100 & 80 & \textbf{0.95} & \textbf{2.57} & \textbf{0.22} & \textbf{0.7442} & \textbf{3.0784} & \textbf{0.0795} & \textbf{0.0613} & \textbf{0.0154} & 0.0389\\
\hline
100 & 160 & \textbf{0.95} & \textbf{2.22} & \textbf{0.22} & \textbf{0.6737} & \textbf{3.0580} & \textbf{0.0786} & \textbf{0.0561} & \textbf{0.0166} & 0.0376\\
\hline
100 & 20 & \textbf{1.00} & \textbf{2.69} & \textbf{0.24} & \textbf{0.7657} & \textbf{2.9984} & \textbf{0.0748} & \textbf{0.0553} & \textbf{0.0134} & 0.0364\\
\hline
100 & 40 & \textbf{1.00} & \textbf{2.17} & \textbf{0.22} & \textbf{0.6612} & \textbf{2.9549} & \textbf{0.0817} & \textbf{0.0596} & \textbf{0.0173} & 0.0332\\
\hline
100 & 80 & \textbf{1.00} & \textbf{3.16} & \textbf{0.29} & \textbf{0.8656} & \textbf{3.0741} & \textbf{0.0748} & \textbf{0.0656} & \textbf{0.0136} & 0.0413\\
\hline
100 & 160 & \textbf{1.00} & \textbf{2.27} & \textbf{0.21} & \textbf{0.6860} & \textbf{3.0085} & \textbf{0.0763} & \textbf{0.0556} & \textbf{0.0158} & 0.0364\\
\hline
200 & 20 & \textbf{0.95} & \textbf{3.69} & \textbf{0.41} & \textbf{0.9121} & \textbf{2.4456} & \textbf{0.0757} & \textbf{0.0865} & \textbf{0.0143} & 0.0293\\
\hline
200 & 40 & \textbf{0.95} & \textbf{3.55} & \textbf{0.34} & \textbf{0.8984} & \textbf{2.4333} & \textbf{0.0800} & \textbf{0.0796} & \textbf{0.0139} & 0.0282\\
\hline
200 & 80 & \textbf{0.95} & \textbf{2.68} & \textbf{0.17} & \textbf{0.7705} & \textbf{2.7630} & \textbf{0.0795} & \textbf{0.0613} & \textbf{0.0154} & 0.0389\\
\hline
200 & 160 & \textbf{0.95} & \textbf{2.47} & \textbf{0.17} & \textbf{0.7302} & \textbf{2.7469} & \textbf{0.0786} & \textbf{0.0561} & \textbf{0.0166} & 0.0376\\
\hline
200 & 20 & \textbf{1.00} & \textbf{2.71} & \textbf{0.12} & \textbf{0.7841} & \textbf{2.7027} & \textbf{0.0748} & \textbf{0.0553} & \textbf{0.0134} & 0.0364\\
\hline
200 & 40 & \textbf{1.00} & \textbf{2.31} & \textbf{0.07} & \textbf{0.7128} & \textbf{2.7296} & \textbf{0.0817} & \textbf{0.0596} & \textbf{0.0173} & 0.0332\\
\hline
200 & 80 & \textbf{1.00} & \textbf{3.15} & \textbf{0.09} & \textbf{0.8636} & \textbf{2.7004} & \textbf{0.0748} & \textbf{0.0656} & \textbf{0.0136} & 0.0413\\
\hline
200 & 160 & \textbf{1.00} & \textbf{2.19} & \textbf{0.03} & \textbf{0.6965} & \textbf{2.7403} & \textbf{0.0763} & \textbf{0.0556} & \textbf{0.0158} & 0.0364\\
\hline
400 & 20 & \textbf{0.95} & \textbf{3.86} & \textbf{0.19} & \textbf{0.9594} & \textbf{2.3610} & \textbf{0.0757} & \textbf{0.0865} & \textbf{0.0143} & 0.0293\\
\hline
400 & 40 & \textbf{0.95} & \textbf{3.86} & \textbf{0.12} & \textbf{0.9669} & \textbf{2.3623} & \textbf{0.0800} & \textbf{0.0796} & \textbf{0.0139} & 0.0282\\
\hline
400 & 80 & \textbf{0.95} & \textbf{3.08} & \textbf{0.05} & \textbf{0.8546} & \textbf{2.5609} & \textbf{0.0795} & \textbf{0.0613} & \textbf{0.0154} & 0.0389\\
\hline
400 & 160 & \textbf{0.95} & \textbf{2.98} & \textbf{0.05} & \textbf{0.8390} & \textbf{2.5517} & \textbf{0.0786} & \textbf{0.0561} & \textbf{0.0166} & 0.0376\\
\hline
400 & 20 & \textbf{1.00} & \textbf{2.74} & \textbf{0.02} & \textbf{0.8008} & \textbf{2.6395} & \textbf{0.0748} & \textbf{0.0553} & \textbf{0.0134} & 0.0364\\
\hline
400 & 40 & \textbf{1.00} & \textbf{2.33} & \textbf{0.00} & \textbf{0.7258} & \textbf{2.6493} & \textbf{0.0817} & \textbf{0.0596} & \textbf{0.0173} & 0.0332\\
\hline
400 & 80 & \textbf{1.00} & \textbf{3.05} & \textbf{0.00} & \textbf{0.8548} & \textbf{2.6504} & \textbf{0.0748} & \textbf{0.0656} & \textbf{0.0136} & 0.0413\\
\hline
400 & 160 & \textbf{1.00} & \textbf{2.34} & \textbf{0.00} & \textbf{0.7297} & \textbf{2.7578} & \textbf{0.0763} & \textbf{0.0556} & \textbf{0.0158} & 0.0364\\
\hline
\end{tabular}
\end{table}

\bibliography{RJreferences.bib}

\address{%
Jieun Shin\\
University of Seoul\\%
Department of Statistical Data Science\\ Seoul, Republic of Korea\\
%
%
\textit{ORCiD: \href{https://orcid.org/0000-1721-1511-1101}{0000-1721-1511-1101}}\\%
\href{mailto:jieunstat@uos.ac.kr}{\nolinkurl{jieunstat@uos.ac.kr}}%
}

\address{%
Changyi Park\\
University of Seoul\\%
Department of Statistics\\
%
%
\textit{ORCiD: \href{https://orcid.org/0000-0002-0912-0225}{0000-0002-0912-0225}}\\%
\href{mailto:bbil@ulm.edu}{\nolinkurl{bbil@ulm.edu}}%
}
